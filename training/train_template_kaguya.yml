name: Kaguya_template_001
use_tb_logger: true
model: srragan #srragan | sr | srgan | ppon | asrragan
scale: 2
gpu_ids: [0]
use_amp: true
use_swa: false

# Dataset options:
datasets:
  train: 
    name: DIV2K
    mode: LRHRC
    dataroot_HR: '../../datasets/train/hr'
    dataroot_LR: '../../datasets/train/lr'
    subset_file: null
    use_shuffle: true
    znorm: false # true | false # To normalize images in [-1, 1] range. Default = None (range [0,1]). Can use with activation function like tanh.
    n_workers: 5 # 0 to disable CPU multithreading, or an integrer representing CPU threads to use for dataloading
    batch_size: 8
    virtual_batch_size: 8
    preprocess: crop # how to process the images after loading
    HR_size: 128 # patch size. Default: 128. Needs to be coordinated with the patch size of the features network
    image_channels: 3 # number of channels to load images in

    # Rotations augmentations:
    use_flip: true # flip images
    use_rot: true # rotate images in 90 degree angles
    use_hrrot: false # rotate images in random degress between -45 and 45
    
  val: 
    name: val_set_20
    mode: LRHROTF
    dataroot_HR: '../../datasets/val/hr'
    dataroot_LR: '../../datasets/val/lr'
    
    znorm: false # true | false # To normalize images in [-1, 1] range. Default = None (range [0,1]). Can use with activation function like tanh.

    # hr_crop: false #disabled
    lr_downscale: false
    lr_downscale_types: ["linear", "bicubic"] # scaling interpolation options, same as in train dataset
path:
    strict: false # true | false 
    root: 'G:/Encodages/Light shit/kaguya-sama-ova-encoding/training/BasicSR'
    # pretrain_model_G: '../../2xESRGAN.pth'

# Generator options:
network_G:
    strict: false # true | false # whether to load the model in strict mode or not
    # ESRGAN:
    which_model_G: RRDB_net # RRDB_net (original ESRGAN arch) | MRRDB_net (modified/"new" arch) | sr_resnet
    norm_type: null
    mode: CNA
    nf: 48 # number of discrim filters in the first conv layer
    nb: 18 # number of RRDB blocks
    nr: 3 #  number of residual layers in each RRDB block
    in_nc: 3 # of input image channels: 3 for RGB and 1 for grayscale
    out_nc: 3 # of output image channels: 3 for RGB and 1 for grayscale
    gc: 32
    group: 1
    convtype: Conv2D # Conv2D | PartialConv2D | DeformConv2D | Conv3D
    net_act: leakyrelu # swish | leakyrelu
    gaussian: true # true | false
    plus: false # true | false
    ##finalact: tanh # Test. Activation function to make outputs fit in [-1, 1] range. Default = None. Coordinate with znorm.


# Discriminator options:
network_D:
    strict: true # true | false # whether to load the model in strict mode or not
    # ESRGAN (default)| PPON:
    which_model_D: discriminator_vgg # discriminator_vgg_128 | discriminator_vgg | discriminator_vgg_128_fea (feature extraction) | discriminator_vgg_fea (feature extraction) | patchgan | multiscale
    norm_type: batch
    act_type: leakyrelu
    mode: CNA # CNA | NAC
    nf: 64
    in_nc: 3
    nlayer: 3 # only for patchgan and multiscale
    num_D: 3 # only for multiscale

# Schedulers options:
train:
    lr_G: 0.0001 # 2e-4 # starting lr_g #Test, default: 1e-4
    weight_decay_G: 0
    beta1_G: 0.9
    lr_D: 0.0001 # 2e-4 # starting lr_d #Test, default: 1e-4
    weight_decay_D: 0
    beta1_D: 0.9

    # For MultiStepLR (ESRGAN, default):
    lr_scheme: MultiStepLR
    # lr_steps: [50000, 100000, 200000, 300000] # training from scratch
    lr_steps_rel: [0.1, 0.2, 0.4, 0.6] # to use lr steps relative to % of training niter instead of fixed lr_steps
    #lr_steps: [50000, 75000, 85000, 100000] #finetuning
    lr_gamma: 0.5 # lr change at every step (multiplied by)


    # For SWA scheduler
    swa_start_iter: 500000 #Just reference: 75% of 500000. Can be any value, including 0 to start right away with a pretrained model.
    # swa_start_iter_rel: 0.75 # to use swa_start_iter relative to % of training niter instead of fixed swa_start_iter
    swa_lr: 1e-4 #Has to be ~order of magnitude of a stable lr for the regular scheduler
    swa_anneal_epochs: 10
    swa_anneal_strategy: "cos"
    
    # Losses:
    pixel_criterion: l1 # "l1" | "l2" | "cb" | "elastic" | "relativel1" | "l1cosinesim" | "clipl1" #pixel loss
    pixel_weight: 1e-2 # 1e-2 | 1
    feature_criterion: l1 # "l1" | "l2" | "cb" | "elastic" #feature loss (VGG feature network)
    feature_weight: 1

    # Adversarial loss:
    gan_type: vanilla # "vanilla" | "wgan-gp" | "lsgan" 
    gan_weight: 5e-3 # * test: 7e-3

    # Other training options:
    # finalcap: clamp # Test. Cap Generator outputs to fit in: [-1, 1] range ("tanh"), rescale tanh to [0,1] range ("scaltanh"), cap ("sigmoid") or clamp ("clamp") to [0,1] range. Default = None. Coordinate with znorm. Required for SPL if using image range [0,1]
    manual_seed: 0
    niter: 500000
    # warmup_iter: -1  # number of warm up iterations, -1 for no warm up
    val_freq: 5000 # 5e3
    # overwrite_val_imgs: true
    # val_comparison: true
    metrics: 'psnr,ssim,lpips' # select from: "psnr,ssim,lpips" or a combination separated by comma ","

logger:
    print_freq: 200
    save_checkpoint_freq: 5000
    overwrite_chkp: false
